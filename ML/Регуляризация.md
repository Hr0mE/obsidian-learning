Математически возникает во время нахождения оптимального по Теореме Гаусса-Маркова #tofill решения уравнения $w = (X^TX)^{-1}X^Ty$, где $X^TX$ - ковариационная матрица фичи-фичи, которая может быть необратима $\Leftrightarrow \exists$ линейно зависимые фичи. 

## Виды регуляризаций
---

Для того, чтобы решение точно существовало, вводят регуляризацию: 
$$
w = (X^TX +\lambda E)^{-1}X^Ty
$$
и рассматривают функцию потерь:
$$
L(f, X, Y) = min_w(||Xw-y||_2^2 + \lambda||w||_k^k)
$$
где:
- $\lambda ||w||_k^k$ -- наложенное ограничение на веса (чтобы они не возрастали неограниченно)
	1. При $k=1 \implies ||w||_k^k = \sum^n_1 |w_i|$ 
	2. При $k=2 \implies ||w||_k^k = \sum^n_1 w_i^2$  
- $\lambda$ - гиперпараметр и подбирается во время обучения. $\lambda \gt 0$ 

***Зам.*** Матрица $(X^TX)^{-1}$ неотрицательно определённая, и прибавляя $\lambda E$ матрица остаётся положительно (неотрицательно) определённой $=>$ при обращении элементы матрицы останутся вещественными

L1-регуляризация ()


| ![[Регуляризация_l1_l2.png]] |
| ------------------------------------ |
| ![[Регуляризация_l1.png]] |

***Зам.*** Существуют нормы $L_x: 0 < x < 1$, однако с ними не очень комфортно работать

## Теорема Гаусса-Маркова
---

$$Y = Xw + \epsilon,\text{ где } \epsilon = [\epsilon_1, \cdots, \epsilon_n]$$
Предположения:
- $E(\epsilon_i) = 0$ $\forall i$
- $Var(\epsilon) = \sigma^2 < \inf$ $\forall i$
- $Cov(\epsilon_i, \epsilon_j) = 0$  $\forall i \neq j$ 

**Тогда:** $w = (X^TX)^{-1}X^Ty$ - оптимальная оценка среди несмещённых. Т.е. Минимизируя эту функцию мы достигнем оптимального решения

***Зам.*** Это предположение для задачи без регуляризации

***Зам.*** Если шум смещён, то это повод задуматься о характере/происхождении данных

