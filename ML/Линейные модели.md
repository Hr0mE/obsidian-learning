## Введение

***Опр.*** Линейными называются функции вида: $y = \sum{w_ix_i} + w_0$  или $y = (\bar{x}, \bar{w}) + w_0$, где 
- $\bar{x} = (x_1, x_2, \cdots, x_D)$ - вектор фичей 
- $\bar{w} = (w_1, w_2, \cdots, w_D)$ - вектор весов (параметров) модели
- $w_0$ - свободный коэффициент/сдвиг (bias) модели

***Зам.*** Полиномиальные и логарифмические модели можно свести к линейным

***Зам.*** Если фича является категориальной, то надо закодировать категории
- One Hot Encoding -- пусть исходный признак мог принимать $M$ значений $𝑐_1$,…,$𝑐_𝑀$. Заменим категориальный признак на 𝑀 признаков, которые принимают значения 0 и 1: 𝑖-й будет отвечать на вопрос «принимает ли признак значение $𝑐_𝑖$​?». 
	- Можно( и *нужно*) убирать одну из фич (т.к. она линейно выразима через иные) $y ∼ w_1​x_1​+…+w_{D−1}​x_{D−1}​+w_{c_1}​​x_{c_1}​​+…+w_{c_M}​​x_{c_M}​​+w \implies$ $y ∼ w_1​x_1​+…+w_{D−1}​x_{D−1}​+(w_{c_1}-w_{c_M})​​_{=w'_{c_1}}x_{c_1}​​+…+(w_{c_{M-1}}-w_{c_M})​​_{=w'_{c_{M-1}}}x_{c_1}​​+w_{c_M}​(x_{c_1} +...+​x_{c_M}​)_{=1}​+w$ = $y ∼ w_1​x_1​+…+w_{D−1}​x_{D−1}​+w'_{c_1}​​x_{c_1}​​+…+w'_{c_{M-1}}​​x_{c_{M-1}}​​+w'_0$ 
- #tofill

***Зам.*** Линейные модели обладают высокой интерпретируемостью. Высокие веса при переменных могут быть в двух случаях:
- Признак является значимым, и малое изменение признака влечёт большое изменение прогноза
- Признак является слишком маленьким по величине, и вес его компенсирует

***Зам*.** Конкретные веса моделей на разных выборках будут разные, однако они будут стремиться к "идеальным" при приближении выборки к генеральной

## Линейная регрессия и метод наименьших квадратов

$MSE(f, X, y) = \frac{1}{N}\sum^N_{i=1}{(y_i-(\bar{x_i}, \bar{w}))^2}$ - среднеквадратичная функция ошибки, функционал

***Опр.*** Функционал - это отображение из множества функций в вещественную ось. Фактически, числовая оценка функции. 

***Зам.*** Из-за подобия вектора предсказаний и вектора таргетов можно ввести расстояние между ними, как функцию потерь. И тогда задача минимизации расстояния между предсказанием и таргетом станет задачей минимизации функции потерь 

**Геометрический подход**
Пусть $𝑥^{(1)},…,𝑥^{(𝐷)}$ – столбцы матрицы 𝑋, то есть столбцы признаков. Тогда
$𝑋𝑤 = 𝑤_1𝑥^{(1)}+…+𝑤_𝐷𝑥^{(𝐷)}$, и задачу регрессии можно сформулировать следующим образом: _найти линейную комбинацию столбцов_ $𝑥^{(1)},…,𝑥^{(𝐷)}$, которая наилучшим способом приближает столбец 𝑦 по евклидовой норме – то есть _найти **проекцию** вектора_ 𝑦 на подпространство, образованное векторами $𝑥^{(1)},…,𝑥^{(𝐷)}$

Разложим вектор $y$ на составляющие:
$y = y_{||} + y_{\bot}$, где 
- $y_{||}$ - проекция $y$ на подпространство векторов $X$
- $y_{\bot}$ - ортогональная составляющая

Т.к. $y_{||} = Xw \implies y_\bot = y - Xw$ и т.к. $X^T (y_\bot) = 0$, то $X^T(y-Xw) = 0 \Leftrightarrow X^Ty = X^TXw \Leftrightarrow w = (X^TX)^{-1}X^Ty$ 

***Зам.*** т.к. $X$ -квадратная, то $X^T$ - тоже квадратная и $\implies (X^TX)$ - невырожденная $\implies$ все столбцы/строки должны быть линейно независимыми. Корреляцию признаков можно посмотреть при помощи seaborn.heatmap(df.corr()). В случае линейной зависимости применяется [[регуляризация]]

**Зам.** Вычислительная сложность аналитического решения $O(D^2N+D^3)$, где
- $D$ - число признаков у одного объекта
- $D^2N$ - сложность перемножения $X^TX$
- $D^3$ - сложность обращения $X^TX$ (нахождения обратной матрицы)
