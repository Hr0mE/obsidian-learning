## Введение
---
Пусть
$$\{(x^{(i)}, y^{(i)})\}_{i=1}^n \text{ - обучающая выборка}$$
$$x^{(i)}\in\mathbb{R}^n \text{, } y^{(i)}\in\{C_i\} \text{, где}$$
- $C_i$ - в общем случае  $C(x)\in \mathbb{R}^n$, но в случае классификации $C(x)\in[0,\cdots, n-1]\subset \mathbb{Z}$ 

***Зам.*** Для бинарной классификации используется $y^{(i)}\in\{-1, +1\}$

### Почему нельзя использовать классификатор и просто брать знак числа?
---
![[LinearClassifier_notRegression.png]]
Потому что тогда мы будем строить прямую, которая наиболее удачным образом проходит через данные, а не разделяющую прямую. 

****Зам.*** #tothink  подумать, как это в формулах выглядит


## Бинарная классификация
---
Бинарную классификацию можно представить в виде разделяющей гиперплоскости, по одну сторону от которой находятся объекты одного класса, а по другую - объекты второго класса. Принадлежность к классу определяется через вектор нормали к плоскости. 

$C(x) = \begin{cases}1 \text{, if } f(x)>0 \\ -1 \text{, if } f(x)<0 \end{cases}$ $\leftrightarrows C(x) = sign(f(x)) = sign(x^Tw)$

Здесь $sign(x^Tw)$ - скалярное произведения вектора признаков $x^T$ на вектор нормали $w$ гиперплоскости. Получаем знак, по какую сторону от гиперплоскости находится объект. 

***Опр.*** Отступом $M^{(i)} = y^{(i)} \cdot f(x^{(i)})$ - называется величина, показывающая, насколько глубоко в *корректном (или некорректном)* классе находится объект.

***Зам.*** Для линейного классификатора $M^{(i)} = y^{(i)} \cdot x^{(i)T}w$ 

$\begin{cases} M^{(i)} > 0 \leftrightarrows y^{(i)} = C(x^{(i)}) \\ M^{(i)} < 0 \leftrightarrows y^{(i)} \neq C(x^{(i)}) \end{cases}$ - отступ.

***Зам.*** т.к. нас интересует, как можно итеративно приближаться к верным весам, то нужна некая функция потерь, которую можно будет минимизировать. Также нужно, чтобы она стремилась к нулю при положительном $M$ (раз уж мы ввели такую удобную функцию) и давала штраф, когда $M$ отрицательно.

Рассмотрим $L_{miss}(y^{(i)}, \hat{y}^{(i)}) = [y^{(i)} \neq \hat{y}^{(i)}] = [M^{(i)} < 0]$, где $[P]$ - оператор Иверсона. (1, если в скобках правда и 0 иначе). Однако у нас получается ступенчатая функция, которая не дифференцируема и не подходит для градиентных методов. Тогда попытаемся минимизировать функцию высшего порядка. #tothink (правильно ли написал или она как-то по-другому называется)

![[LinearClassifier_squareLoss.png]]

Однако появляется проблема - чем больше у нас уверенности в принадлежности объекта к правильному классу, тем функция сильнее нас штрафует (после достижения минимума). Решение - другие функции потерь!

![[LinearClassifier_otherLoss.png]]

## Логистическая регрессия
---
***Опр.*** Логистическая регрессия - метод классификации, который оценивает **вероятность** принадлежности объекта к определённому классу.

Задача: перевести вектор $x \in\mathbb{R}^n$ в $P(z) \in[0, 1]$
Решение: использовать сигмоиду $\sigma(z) = \frac{1}{1+exp(-z)}$, где $z \in \mathbb{R}$ 

![[LinearClassifier_sigmoid.png]]

***Зам.*** В большинстве современных логистических регрессоров или нейросетях (в качестве функции активации) используется другие функции, а не сигмоида.

***Зам.*** Разделяющая поверхность остаётся линейной, однако сигмоида - нелинейна, потому что она предсказывает **вероятность** нахождения объекта в определённом классе. *(Перестаём отображать удалённость точки от разделяющей поверхности линейно и приводим всё к вероятностям)*

## Метод опорных векторов  SVM
Конспект: https://education.yandex.ru/handbook/ml/article/linear-models#:~:text=%D0%BE%D0%B4%D0%B8%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2%D1%8B%D0%B9%20%D0%BD%D1%83%D0%BB%D0%B5%D0%B2%D0%BE%D0%B9%20%D0%BB%D0%BE%D1%81%D1%81%3A-,Hinge%20loss%2C%20SVM,-%D0%94%D0%BB%D1%8F%20%D1%82%D0%B0%D0%BA%D0%B8%D1%85%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B5%D0%B2
### Откуда берётся сигмоида?
---
****Зам.*** Более глубокое понимание, почему математически появляется именно сигмоида и почему логистическая функция потерь является корректной оценкой сверху для бинарного классификатора на слайде. Шаги (2) и (3) мы просто приравняли друг к другу т.к. это объекты из одного множества и существует некоторое биективное отображение между ними (-> можно поставить знак равенства).

![[LinearClassification_logisticIntuition.png]]

## Мультиклассовая классификация
---
***Опр.*** Мультиклассовая классификация - это способность модели разделять данные на более, чем 2 класса.

*Примеры:*
- *Классификация учеников на отличников, хорошистов, троечников и неуспевающих на основе оценок по всем предметам и некоторых личностных характеристик (модель навешивания ярлыков на учеников).*
- *Классификация звёзд по классам в зависимости от светимости, температуры, размеров и пр.* 
- *Классификация текста по его настроению.*

***Зам.*** Не путать с моделями многозадачной классификации, которые позволяют выполнять одновременно несколько задач. *(Например: определение объектов на дороге, их классификация и предсказание траектории движения).*  

***Зам.*** Для мультиклассовой классификации используются свои [[Метрики в мультиклассовой классификации|метрики]]


### One vs Rest
---
Каждый класс противопоставляется всем остальным вместе взятым. Классификатор создаётся для каждого класса. Для каждого объекта строится вероятность принадлежности его к $i$-тому классу, а потом берётся максимальная вероятность.

***Зам.*** Обучение происходит на всей выборке.

![[LinearClassifier_multiclass.png]]


### One vs One
---
Каждый класс по очереди противопоставляется всем остальным по отдельности. Классификатор строится для каждой **границы** между классами (для каждой пары классов). 

***Зам.*** Обучение для $i$-того класса происходит на своей подвыборке. (что не очень хорошо для нейросетевых моделей)


| ![[LinearClassification_one_vs_one.png]] |
| ---------------------------------------- |
| ![[LinearClassification_multuSummary.png]]     |

### Softmax регрессия
---
Обобщение логистической регрессии на задачу многоклассовой классификации. #tofill 

### Деревья принятия решения и случайный лес
---
#tofill 

### Нейросетевые модели
---
Используются GNN и CNN. #tofill 