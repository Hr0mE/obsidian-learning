***Опр.*** Ординальными признаками называются ранжированные дискретные значения. *Например, номер курса, класс химической опасности, номер этажа в доме*
 
Для большинства сегодняшних задач достаточно знать:
1. Градиентный бустинг на решающих деревьях
2. Нейросетевые модели

Однако когда мы говорим о любых объектах, где следующее входное значение не зависит (или слабо зависит) от предыдущего, то лучшим выбором будут простые модели - [[ML/ML от Яндекса/Линейные модели/Регрессия]]. Антипримером будет распознавание объектов на изображении, поскольку значение в каждом пикселе сильно зависит от значения в соседних. Второй антипример – анализ временных рядов.

***Зам.*** По сути, процесс обучение на выборке - это процесс переобучения модели на этой выборке. И чем ближе обучающая выборка к генеральной, тем точнее к истинному значению "переобучится" модель.

***Зам.*** [[Метрики и оценка качества модели|Метрика]] качества предсказания модели должна отличаться от [[0 Функционалы потерь|функции потерь]]! *(переобучение, не дифференцируемость)*

***Зам.*** В реальном мире данные, а именно их распределение претерпевает значительное изменение после каждого сильного потрясения. *(Пример: средний возраст заведения детей явно сдвинется при глобальном катаклизме/кризисе)*

Градиентный спуск - один из методов оптимизации функции потерь #tofill 
$w_{t+1}​=w_t​−α∇_w​L,$ где 
- $w_t$ - параметры модели на $t$-том шаге
- $\alpha$ - скорость обучения (шаг градиента)
- $∇_w​L$ - вектор градиента от функции потерь $L$ по параметрам $w$

![[example_regression.png]]
![[example_bin_classify.png]]


-----
## Практикум

Скорость numpy достигается за счёт веторизации и броадкастинга
```
m[i][j] # обращение сначала к строке, а следом к элементу
m[i, j] # взятие сразу нужного элемента
```
```
q1 = m[:, :] # возвращает view!
q2 = m[[:, :]] # возвращает копию!

>> q1.base # Смотрим на то, является ли view, и если да, то возвращаем прообраз
m
>> q2.base # А если нет, то возвращаем None
None
```
---

## Важно
Помимо технической части, ml-специалисту также важно уметь интегрировать своё  исследование (или разработку отдельной модели) в пайплайн работы всей команды. Правила координации работы команды data science, а также все best practices излагаются дисциплиной [[Введение в ML-OPS | ML-OPS]]
. 

