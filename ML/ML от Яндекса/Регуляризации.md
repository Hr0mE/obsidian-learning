Математически возникает во время нахождения оптимального по [[#Теорема Гаусса-Маркова|теореме Гаусса-Маркова]] решения уравнения $w = (X^TX)^{-1}X^Ty$, где $X^TX$ - ковариационная матрица фичи-фичи, которая может быть необратима $\Leftrightarrow \exists$ линейно зависимые фичи. 

## Виды регуляризаций
---

Для того, чтобы решение точно существовало, вводят регуляризацию: 
$$
w = (X^TX +\lambda E)^{-1}X^Ty
$$
и рассматривают функцию потерь:
$$
L(f, X, Y) = min_w(||Xw-y||_2^2 + \lambda||w||_k^k)
$$
где:
- $\lambda ||w||_k^k$ -- наложенное ограничение на веса (чтобы они не возрастали неограниченно)
	1. При $k=1 \implies ||w||_k^k = \sum^n_1 |w_i|$  - $L_1$-регуляризация
	2. При $k=2 \implies ||w||_k^k = \sum^n_1 w_i^2$  - $L_2$-регуляризация
- $\lambda$ - гиперпараметр и подбирается во время обучения. $\lambda \gt 0$ *(чем больше $\lambda$, тем строже идёт отбор весов. При малом $\lambda$ существует риск переобучения. Обычно перебирается по логарифмической шкале)*

***Зам.*** Матрица $(X^TX)^{-1}$ неотрицательно определённая *(разложение Холецкого)*, и прибавляя $\lambda E$ матрица остаётся положительно (неотрицательно) определённой $=>$ при обращении элементы матрицы останутся вещественными

***Зам.*** Регуляризацию можно применять не только с $MSE$, но и с другими [[Функционалы потерь|функционалами потерь]]. Однако аналитическое решение существует лишь для $MSE + L_2$ #tothink 

**$L_1$-регуляризация (Lasso):**
1. Штрафует большое отклонение наравне с небольшим *(=> стабильнее поиск минимума)*
2. Помогает отбирать не значимые признаки *([[#^7cadf8|См.картинки]] - минимум будет на одной из осей)*
3. Существуют случаи, когда $min(L_1) > min(L_2)$

***Зам.*** Из-за недифференцируемости определяем $f'(0)=0$ 

**$L_2$-регуляризация (Ridge):**
1. Штрафует большое отклонение больше, чем за малое *(больше шаг на первоначальном этапе)*
2. Дифференцируема
3. Помогает сохранить мультиколлинеарность

***Зам.*** Существуют нормы $L_x: 0 < x < 1$, однако с ними не очень комфортно работать

| ![[Регуляризация_l1_l2.png]] |
| ---------------------------- |
| ![[Регуляризация_l1.png]]    |

^7cadf8
***Комментарий.*** Отбор признаков происходит за счёт того, что $min$ достигается при $w_{i_{k}}= 0$. Это происходит из-за $\nabla L_1$-регуляризационного члена - он равен нулю только в нуле. В противном случае он будет пинать в сторону нуля с одинаковой силой вне зависимости от его удалённости. *(в отличие от $\nabla L_2$)* 

****Зам.*** Т.к. $\nabla f = \nabla(||Xw-y||_2^2 + \lambda||w||_1^1)$ = $\nabla(||Xw-y||_2^2) + \nabla(\lambda||w||_1^1)$ = $2X^T(Xw-y) + \lambda sign(w)$ - это сумма двух градиентов, то они "перетягивают" градиент минимизируемой функции $f$. И когда $w_{i_k}$ смещается от центра, оно уходит из положения равновесия, и градиент функции потерь "передавливает" минимум ближе к положению, где $w_{i_k} = 0$. Оптимум будет долго оставаться в этой точке потому, что в любой иной точке, где функция потерь достигает того же значения сумма весов будет больше.


## Теорема Гаусса-Маркова
---

$$Y = Xw + \epsilon,\text{ где } \epsilon = [\epsilon_1, \cdots, \epsilon_n]$$
Предположения:
- $E(\epsilon_i) = 0$ $\forall i$
- $Var(\epsilon) = \sigma^2 < \inf$ $\forall i$
- $Cov(\epsilon_i, \epsilon_j) = 0$  $\forall i \neq j$ 

**Тогда:** $w = (X^TX)^{-1}X^Ty$ - оптимальная оценка среди несмещённых. Т.е. Минимизируя эту функцию мы достигнем оптимального решения

***Зам.*** Это предположение для задачи без регуляризации

***Зам.*** Если шум смещён, то это повод задуматься о характере/происхождении данных
