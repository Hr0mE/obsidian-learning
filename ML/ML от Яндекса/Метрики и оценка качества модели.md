***Опр.*** Метрика - это #tofill 

***Опр.*** Оценка качества модели - это #tofill 

***Конспект статьи:*** https://habr.com/ru/articles/760550/

## Разбиение датасета
---
Датасет разбивается на 3 части:
1. Тренировочная (70%) - обучение модели
2. Валидационная (20%) - проверяем качество модели после каждого шага обучения. Используются .
3. Тестовая (10%) - проверка итоговой модели. Если нас устраивает качество, то модель заливается в прод, если нет, то обучение происходит с начала. Используются метрики качества модели.

## Cross validation
---
![[ModelEvaluation_CV.jpg]]

При кросс-валидации выборка разбивается на $n$ фолдов, затем происходит обучение модели на $(n-1)_i$ фолдах и оценивается качество на оставшемся *валидационном* фолде$\forall i \in\overline{[1, n]}$. 

Затем ищется средняя арифметическая ошибка по всем итерациям - она и считается конечной. Гиперпараметры тоже ищутся через среднее арифметическое (ЦПД говорит о том, что параметры стремятся к идеальным)

***Зам.*** Перед заливанием в прод нужно обучить модель на всём тренировочном + валидационном датасете с гиперпараметрами, выбранными на предыдущем шаге. Дальше тестовая выборка и прод.

## Метрики качества регрессии
---
Метрики качества нужны для определения итогового качества модели и используется для интерпретации человеком её жизнеспособности. #tofill 

### RMSE
---
$$RMSE = \sqrt{\frac{1}{n}\sum^{n}_{i=1}(y - \hat{y})^2}$$
Зачем нужно?
- Переводит MSE в единицы измерения вектора $y$

### MAPE
---
$$MAPE = \frac{1}{n}\sum_{i=1}^n\frac{|y - \hat{y}|}{|y|} \cdot 100\%$$
Зачем нужно?
- Показывает разницу между объектами в процентах. *Например, если мы предсказываем $100$ вместо $200$ - это не то же самое, если мы будем предсказывать $10100$ вместо $10200$.*

***Зам.*** Оптимизируется с помощью MAE

### $R^2$
---
$$R^2 = 1 - \frac{\sum_{i=1}^n(y-\hat{y})^2}{\sum_{i=1}^n(y-\bar{y})^2}$$
Где:
- $\bar{y}$ - константное решение, предсказание средним арифметическим. *Baseline*.
- $R^2 \in (-\inf; 1]$, где $R^2 = 1$ - идеальное решение, $R^2(\bar{y}) = 0$

Зачем нужно?
-  Показывает, насколько построенная модель лучше/хуже baseline.

***Зам.*** Оптимизируется с помощью MSE

## Метрики качества классификации
---

### Accuracy
***Опр.*** Accuracy - доля верных ответов. 
$$Accuracy = \frac{1}{n}\sum_{i=1}^n[y^{(i)} = \hat{y}^{(i)}], \text{ где:}$$
- $[P]$ - оператор Иверсона (1, если в скобках истина; 0 иначе)

***Зам.*** Базовый accuracy не устойчив к дисбалансу классов. Если один из классов представлен 99 объектами, а второй 1, и предсказывать только первый класс, то `accuracy` будет 99%. Для этого вводят `balanced accuracy`

$$Balanced\text{ }accuracy = \frac{1}{C}\sum_{k=1}^C\frac{\sum_i[y^{(i)} = k \text{ and }y^{(i)} = \hat{y}^{(i)}]}{\sum_i[y^{(i)} = k]}, \text{ где:}$$
В примере выше, `balanced accuracy` даст результат в `0.5`

### Precision, Recall и F-score
***Опр.*** Precision - точность 
***Опр.*** Recall - полнота
***Опр.*** F-score - гармоническое среднее между precision и recall


***Опр.*** ROC-Curve - 
***Опр.*** PR-Curve - 

