***Опр.*** Ординальными признаками называются ранжированные дискретные значения. *Например, номер курса, класс химической опасности, номер этажа в доме*
 
Для большинства сегодняшних задач достаточно знать:
1. Градиентный бустинг на решающих деревьях
2. Нейросетевые модели

Однако когда мы говорим о любых объектах, где следующее входное значение не зависит (или слабо зависит) от предыдущего, то лучшим выбором будут простые модели - [[Линейные модели]]. Антипримером будет распознавание объектов на изображении, поскольку значение в каждом пикселе сильно зависит от значения в соседних. Второй антипример – анализ временных рядов.

***Зам.*** Метрика качества предсказания модели должна отличаться от функции потерь! *(переобучение, не дифференцируемость)*

Градиентный спуск - один из методов оптимизации функции потерь #tofill 
$w_{t+1}​=w_t​−α∇_w​L,$ где 
- $w_t$ - параметры модели на $t$-том шаге
- $\alpha$ - скорость обучения (шаг градиента)
- $∇_w​L$ - вектор градиента от функции потерь $L$ по параметрам $w$

![[example_regression.png]]
![[example_bin_classify.png]]

-----
## Практикум

Скорость numpy достигается за счёт веторизации и броадкастинга
```
m[i][j] # обращение сначала к строке, а следом к элементу
m[i, j] # взятие сразу нужного элемента
```
```
q1 = m[:, :] # возвращает view!
q2 = m[[:, :]] # возвращает копию!

>> q1.base # Смотрим на то, является ли view, и если да, то возвращаем прообраз
m
>> q2.base # А если нет, то возвращаем None
None
```
---

## Важно
Помимо технической части, ml-специалисту также важно уметь интегрировать своё  исследование (или разработку отдельной модели) в пайплайн работы всей команды. Правила координации работы команды data science, а также все best practices излагаются дисциплиной [[0 ML-OPS]]. 