Пусть
$$\{(x^{(i)}, y^{(i)})\}_{i=1}^n \text{ - обучающая выборка}$$
$$x^{(i)}\in\mathbb{R}^n \text{, } y^{(i)}\in\{C_i\} \text{, где}$$
- $C_i$ - в общем случае  $C(x)\in \mathbb{R}^n$, но в случае классификации $C(x)\in[0,\cdots, n-1]\subset \mathbb{Z}$ 

***Зам.*** Для бинарной классификации используется $y^{(i)}\in\{-1, +1\}$

## Почему нельзя использовать классификатор и просто брать знак числа?
---
![[LinearClassifier_notRegression.png]]
Потому что тогда мы будем строить прямую, которая наиболее удачным образом проходит через данные, а не разделяющую прямую. 

****Зам.*** #tothink  подумать, как это в формулах выглядит


## Бинарная классификация
---
Бинарную классификацию можно представить в виде разделяющей гиперплоскости, по одну сторону от которой находятся объекты одного класса, а по другую - объекты второго класса. Принадлежность к классу определяется через вектор нормали к плоскости. 

$C(x) = \begin{cases}1 \text{, if } f(x)>0 \\ -1 \text{, if } f(x)<0 \end{cases}$ $\leftrightarrows C(x) = sign(f(x)) = sign(x^Tw)$

Здесь $sign(x^Tw)$ - скалярное произведения вектора признаков на вектор нормали гиперплоскости. Получаем знак, по какую сторону от гиперплоскости находится объект. 

***Опр.*** Отступом $M^{(i)} = y^{(i)} \cdot f(x^{(i)})$ - называется величина, показывающая, насколько глубоко в *корректном (или некорректном)* классе находится объект.

***Зам.*** Для линейного классификатора $M^{(i)} = y^{(i)} \cdot x^{(i)T}w$ 

$\begin{cases} M^{(i)} > 0 \leftrightarrows y^{(i)} = C(x^{(i)}) \\ M^{(i)} < 0 \leftrightarrows y^{(i)} \neq C(x^{(i)}) \end{cases}$ - отступ.

***Зам.*** т.к. нас интересует, как можно итеративно приближаться к верным весам, то нужна некая функция потерь, которую можно будет минимизировать. Также нужно, чтобы она стремилась к нулю при положительном $M$ (раз уж мы ввели такую удобную функцию) и давала штраф, когда $M$ отрицательно.

Рассмотрим $L_{miss}(y^{(i)}, \hat{y}^{(i)}) = [y^{(i)} \neq \hat{y}^{(i)}] = [M^{(i)} < 0]$, где $[P]$ - оператор Иверсона. (1, если в скобках правда и 0 иначе). Однако у нас получается ступенчатая функция, которая не дифференцируема и не подходит для градиентных методов. Тогда попытаемся минимизировать функцию высшего порядка. #tothink (правильно ли написал или она как-то по-другому называется)

![[LinearClassifier_squareLoss.png]]

Однако появляется проблема - чем больше у нас уверенности в принадлежности объекта к правильному классу, тем функция сильнее нас штрафует (после достижения минимума). Решение - другие функции потерь!

![[LinearClassifier_otherLoss.png]]

## Логистическая регрессия
---
***Опр.*** Логистическая регрессия - метод классификации, который оценивает **вероятность** принадлежности объекта к определённому классу.

